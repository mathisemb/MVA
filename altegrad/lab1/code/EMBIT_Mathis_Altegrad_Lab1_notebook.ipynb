{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 1: NMT</h2><h3> Neural Machine Translation</h3> 10 / 10 / 2023<br> Dr. G. Shang and H. Abdine</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        # you should return a tensor of shape (seq, batch, feat)\n",
        "\n",
        "        # h0 = zeros of right size if not provided\n",
        "        # then rnn applies automatically on the whole sequence as input contains the full sequence (dim=0)\n",
        "        output, hn = self.rnn(self.embedding(input))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "outputs": [],
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "\n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = self.ff_concat(torch.cat((target_h_rep, source_hs), dim=2))\n",
        "        scores = self.ff_score(torch.tanh(concat_output)) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n",
        "        return ct, norm_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "        embeddings = self.embedding(input)\n",
        "        ht, hn = self.rnn(embeddings, h) # this time we also provide h, the previous hidden state\n",
        "        tilde_h = torch.tanh(self.ff_concat(torch.cat((source_context, ht), dim=2)))\n",
        "        prediction = self.predict(tilde_h)\n",
        "        #prediction = torch.softmax(prediction, dim=2)\n",
        "        return prediction, ht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "outputs": [],
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "\n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "\n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "\n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "\n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod:\n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "\n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        scores_grid = []\n",
        "\n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context, norm_scores = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "                scores_grid.append(norm_scores)\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            target_input = torch.argmax(prediction, dim=2) # the predicted word\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "\n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
        "\n",
        "        return to_return, scores_grid\n",
        "\n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "\n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "\n",
        "                        unnormalized_logits, _ = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()\n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n",
        "                        pbar.set_postfix(tdqm_dict)\n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "\n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "\n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "\n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "\n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints\n",
        "\n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "\n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits, scores_grid = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl), scores_grid\n",
        "\n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "\n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5RprtnBK-ia"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "## Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "datl5SFtJ9Br"
      },
      "outputs": [],
      "source": [
        "!wget -c \"https://public.am.files.1drv.com/y4mVTt6UldlF8GkfCg7gkjeTPHBa7LTERKQFNO06VzMQvF6BqDb0KRu-mI8OnJleSZz_7uuHyKKHu9wmSiVZdZByswmAmq-LDUUjeeUHJBnQoLbQJ9VgBUvmgbCeMlFY_GEYfnk6I6qJwI3_6INLXG14zN8rgC5nLc6ifJwE1aYmuuRv67FVz5_8Tyqv48kX-D29GDkUjwd82cGSQUt0Mg5RpnnxPchRsYyP-b67TBLlo4\" -O \"data.zip\"\n",
        "!wget -c \"https://public.am.files.1drv.com/y4m1-A7S0GAgV5jxk95GyDONth5vV94kCDgd3VDvDz-QSnM8x29h2H2_CPQOxwJn0i4scosfVWDR2_O7nAhc8DOC59gte380n-Iiut-Hc-XgbSGC5X4bB8dPBdsNfz1dKL9CZ5-VOD8kgm1JhBLi22ACiIGK28XeeJwY9-8gF7G_hMwK5SvyVM6LGQDxojNo3pB6LA5cMT4USfuDl3orn9YR3rZBUwQMx6c1HR7PgmCoXk\" -O \"pretrained_moodle.pt\"\n",
        "!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "## Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSZ-cvSuLQVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "c624504c-0ce5-40b2-87e4-b81513571b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded\n",
            "data prepared\n",
            "= = = attention-based model?: True = = =\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch : 0/19: 100%|█████████████████| 2.13k/2.13k [01:47<00:00, 19.8it/s, loss=5.17, test loss=4.66]\n",
            "Epoch : 1/19: 100%|█████████████████| 2.13k/2.13k [01:46<00:00, 20.1it/s, loss=4.45, test loss=4.22]\n",
            "Epoch : 2/19: 100%|█████████████████| 2.13k/2.13k [01:47<00:00, 19.9it/s, loss=4.05, test loss=3.88]\n",
            "Epoch : 3/19:  52%|████████▉        | 1.12k/2.13k [00:52<00:47, 21.2it/s, loss=3.81, test loss=3.88]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b377f2078111>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                          max_size=30) # max size of generated sentence in prediction mode\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save_models\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'my_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-67d07a3db367>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience)\u001b[0m\n\u001b[1;32m    132\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mloader_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flush gradient attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                             \u001b[0msentence_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = False # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "\n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "\n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "\n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "\n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "\n",
        "    print('data loaded')\n",
        "\n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "\n",
        "    print('data prepared')\n",
        "\n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "\n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "\n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(path_to_save_models + 'my_model.pt')"
      ],
      "metadata": {
        "id": "1kG81lplSrNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "## Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCvZmwWoCTUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17feb33-9650-46a1-d308-d25ccc4e6b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXbQjP_YrgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d165bad-bf2f-4221-91e6-93e3d91085f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean -> elle est tellement méchant méchant . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas empêcher de de fumer fumer fumer fumer fumer fumer fumer fumer fumer fumer urgence urgence urgence urgence urgence urgence . urgence urgence . urgence urgence .\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants jouent cache cache cache cache caché caché caché caché caché caché caché caché caché caché caché caché caché caché caché dentifrice perdre caché risques rapide caché risques éveillés\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat s est en du du pression peigne peigne cheminée portail portail portail portail portail portail portail portail indépendant oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux\n"
          ]
        }
      ],
      "source": [
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt') # 'my_model.pt'\n",
        "\n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "\n",
        "    for elt in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source/target alignments plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt') # 'my_model.pt'\n",
        "    sentence = 'She is so mean .'\n",
        "    prediction, scores_grid = model.predict(sentence)\n",
        "    print('= = = = = \\n','%s -> %s' % (sentence, prediction))\n",
        "    result = []\n",
        "    for distrib in scores_grid:\n",
        "      line = []\n",
        "      for score in distrib:\n",
        "        line.append(score.item())\n",
        "      result.append(line)\n",
        "    #print(result)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(result)\n",
        "\n",
        "source = sentence.split()\n",
        "target = prediction.split()\n",
        "\n",
        "ax.set_xticks(range(len(source)), source)\n",
        "ax.set_yticks(range(len(target)), target)\n",
        "ax.set_title('source/target alignments')\n",
        "ax.set_xlabel('English sentence (source)')\n",
        "ax.set_ylabel('French sentence (target)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "7dEvxyOlwMQg",
        "outputId": "f7f0aa0b-638a-42c5-bfeb-0f90e978d9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " She is so mean . -> elle est tellement méchant . <EOS>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lAYRLWYIh5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}